# Q&Aカンペ（CaShield）

発表現場でそのまま使えるよう、要点だけを簡潔にまとめました。

## Q1. 要約プロンプトの工夫は？
- 役割を明示（監視AIとしての視点を固定）
- 構造化JSONで出力（ng_word/turns/summary/severity/action/comfort）
- 事実ベース・誇張禁止・丁寧なトーンを指示

## Q2. レイテンシはどのくらい？
- 入力〜VAD: 数十ms
- 文字起こし: 発話の長さに依存（数秒発話で数秒）
- LLM要約: バックグラウンドで数秒
- 体感（警告音まで）: だいたい3〜5秒

## Q3. NGワードの管理と表記ゆれ対策は？
- 語彙はテキストで管理、編集容易
- かな正規化＋類似度（部分一致）で検出
- 短すぎる語は除外、深刻度が高い語だけ警告音

## Q4. なぜこの音声処理構成？
- 低遅延な録音と軽量VADで、話している部分だけを効率処理
- リアルタイム性と安定性を最優先

## Q5. 文字起こし（ASR）の“範囲・時間”はどう決める？
- 20msの細切れで録音し、VADで「1発話」を切り出す
- 文頭/文末に前後パディング（頭切れ・語尾欠けを抑制）
- 無音が続いたら確定、長すぎる発話は安全のため強制確定
- ログは「FASTで追記→FINALで同じIDを上書き確定」
- 用語: 「1発話」はVADで区切られた連続音声ひとかたまり。「2発話」はその単位を2つ分。
- 調整の目安:
  - 語尾が欠ける→後パディングを増やす／攻撃性を下げる
  - ノイズが多い→攻撃性を上げる／フレーム長を長くする
  - 遅延が気になる→パディングを少し縮める

## Q6. LLM要約の“範囲・時間”はどう決める？
- NG行を基点に、まず前後2発話（=VADユニット2つ）を初期窓にする
- 合計が最低秒数（1.5秒）に届くまで前後へ広げる
- 上限秒数（10秒）やトークン上限（1500）を超えたら端から削って収める
- NG行の時刻を“アンカー”として保持し、要約カードを原文に紐付け
- 調整の目安:
  - 文脈が足りない→最低秒数を増やす
  - 冗長→上限秒数やトークン上限を下げる
  - 文体・トーン→プロンプトの指示を調整

## Q7. なぜ Faster-Whisper を採用？ログ処理でも？
- CPU中心でも実用速度・省メモリ（int8/最適化実装）。Raspberry Pi でも現実的に動く。
- 同等精度帯で安定。GPUを前提にせず、依存が軽く配布が容易。
- FAST/FINALを同一エンジンで統一し、一貫した出力と保守性を確保（再現性・検証が楽）。
- ログ（時間制約が緩い処理）でも同じエンジンに統一することで、結果の揺れや依存の重複を避ける（学習/運用コストを削減）。
- 将来は設定だけでモデル/量子化を切替可能（柔軟に精度↔速度を選べる）。

## Q8. VADで発話を切り出す“基準”は？
- 20msごとの短い区切りで「声/無音」を判定する。
- 連続して「声」と判定された時点で発話開始。頭が欠けないように前側に少し余白（約240ms）を付ける。
- 連続して「無音」が一定時間（約500ms）続いたら発話終了。語尾が切れないように後側に余白を残す。
- 雑音環境では判定を厳しく、静かな環境では緩くできる（攻撃性の調整）。
- 異常に長い発話は安全のため一定長で区切る（フェイルセーフ）。

## Q9. 大まかなシステム構成は？
- 音声入力: マイクから低レイテンシで取り込み。
- 前処理: VADで「1発話」に切り出し（前後に少し余白）。
- 文字起こし: FASTで即時テキスト化→ログ追記→KWS→必要なら警告音。FINALで確定し、同じ行を置換・再判定。
- 検出: かな正規化＋類似度でNGを判定。短語は除外、深刻度が高い語だけ鳴動。
- 要約: 重要部分だけを抽出してLLMで要約・対応・慰めを作成し、逐次保存。
- 表示: Webで原文（追記はSSE）と要約カードを閲覧。
- 設定: すべてコード内に集約（モデル/しきい値/ビーム/鍵等）。

## Q10. 要約の構造化で、なぜJSON？（MarkdownやYAMLではなく）
- 機械可読で厳密。生成の揺れが少なく、逸脱を検知しやすい。
- 1行に収まり、JSONLとしてそのまま保存・配信しやすい（SSE/HTTPとも相性が良い）。
- 主要言語・ブラウザで標準サポートが充実。実装・保守コストが低い。
- Markdownは曖昧、YAMLはインデント依存で崩れやすい（LLM出力で誤りが出やすい）。本用途では機械処理を最優先。

## Q11. 0ms前提の根拠や代替は？
- 0msは理論値で、実運用では入出力やOSの遅延が必ず乗る。
- 目標は「追加待ち時間を最小化」で、固定の小さな余白で揺らぎを抑える。
- 代替は20ms前後の処理粒度＋前後の最小パディングで実効を取る。
- 即時はFAST、確定はFINALで追随し、体感の滑らかさを優先する。

## Q12. 使用技術はどんなもの？
- faster-whisper: 軽量なASR。量子化と最適化でCPUでも実用速度をねらい、短い発話を低遅延で処理しやすい。
- CTranslate2: 推論高速化ランタイム。INT8などの量子化で省メモリ・高スループットを実現し、CPU/GPUを柔軟に活用する。
- sounddevice: クロスプラットフォームのオーディオ入出力。ストリーミング録音/再生を低レイテンシで扱える。
- WebRTC VAD: 音声区間検出。短いフレームで「声/無音」を判定し、発話の開始/終了を安定して見極める。
- pykakasi: 日本語の読み変換（かな/ローマ字）。正規化で表記ゆれを抑え、検出の精度を底上げする。
- rapidfuzz: 高速なあいまい一致。部分一致・類似度スコアを用い、閾値や短語除外で誤警報を抑制する。
- Flask: 軽量Webフレームワーク。ログ/要約の表示や簡易APIの提供に向く。
- google-genai: 生成AIクライアント。要約などのテキスト生成を扱い、モデルや温度・長さを調整できる。
- Threading/asyncio: 並行処理の基盤。録音・認識・検出・保存を分担し、待ち時間を隠して全体の体感を改善する。
- NumPy: 数値配列ライブラリ。音声バッファ変換や統計処理を効率よく実装できる。

## Q13. 音声バッファ変換はこのシステムでどう使われる？
- 入力の型・スケール・サンプリングレート・チャンネル数を下流の前提に揃え、安定した処理条件をつくる。
- 連続列を短いフレームへ切り分け、区間検出や文字起こしに適した粒度にする。
- 不要なコピーを避けて並行処理に渡し、計算負荷と遅延を抑える（体感の滑らかさを維持）。
