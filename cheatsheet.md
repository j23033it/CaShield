# ゼミ発表用チートシート

## 発表台本案

---

### 1. はじめに (スライド1: タイトル)

皆さん、こんにちは。本日は「リアルタイム音声認識とLLMを活用した対人業務支援AI『CaShield』」というテーマで発表します。

このシステムは、特に小売店やコールセンターなどで発生する、いわゆる「カスタマーハラスメント」の対応負荷を軽減することを目的として開発しました。

---

### 2. 背景と目的 (スライド2: 問題提起)

(イラストなどを見せながら)
対人業務の現場では、時に厳しい言葉を受けることがあります。これにより、従業員は大きな精神的ストレスを抱え、離職の一因となることも少なくありません。

そこで私たちは、テクノロジーを用いてこの問題を解決できないかと考えました。
目的は3つです。
1.  **その場での抑止**: 攻撃的な言動を検知し、その場で警告することで状況の沈静化を促します。
2.  **担当者の精神的負担の軽減**: システムが客観的な記録を取ることで、「自分がすべて受け止めなければ」というプレッシャーを和らげます。
3.  **事後対応の効率化**: 会話が自動で記録・要約されるため、報告書作成の手間が省け、事実に基づいた迅速な対応が可能になります。

---

### 3. システム概要 (スライド3: 全体構成図)

こちらがシステムの全体像です。大きく分けて3つのプロセスが連携して動作します。

1.  **リアルタイム監視プロセス (`rt_stream.py`)**:
    *   マイクからリアルタイムで音声を取得し、無音区間を除去します。
    *   音声認識エンジンで文字に変換します。
    *   あらかじめ設定したNGワードが含まれているかをチェックします。
    *   検知した場合、警告音を鳴らし、会話の生ログをファイルに書き出します。

2.  **LLM要約プロセス (`llm_worker.py`)**:
    *   生ログファイルを監視し、NGワードが記録されると起動します。
    *   NGワードが記録された会話の前後を自動的に切り出し、LLM（本研究ではGoogleのGemini）に送信します。
    *   LLMが、会話の要約、深刻度、推奨対応などを構造化データ（JSON形式）で生成し、別のファイルに保存します。

3.  **Webアプリケーション (`webapp/app.py`)**:
    *   記録された生ログと、LLMによる要約を閲覧するためのWebインターフェースです。
    *   これにより、管理者はいつでもどこでも状況を確認できます。

これらのプロセスは独立して動くため、例えば音声認識が停止しても、過去のログの要約は継続されるなど、堅牢な設計になっています。

---

### 4. 技術解説1: 音声認識の仕組み (スライド4: 音声認識フロー)

次に、このシステムの中核である「音声認識からNGワード検知まで」の流れを詳しく説明します。

1.  **音声入力 (`sounddevice`)**: まず、低遅延な音声ライブラリである`sounddevice`を使い、マイクから音声データを取得します。ここでは、一般的な16kHzモノラル形式でデータを取り込みます。

2.  **音声区間検出 (VAD - `webrtc-vad`)**: 次に、Googleが開発したWebRTCのVAD技術を使い、「人が話している区間」だけを切り出します。これにより、無音のデータを処理し続ける無駄をなくし、認識精度と効率を向上させています。

3.  **文字起こし (ASR - `faster-whisper`)**: 切り出された音声データを、OpenAIのWhisperを高速化した`faster-whisper`というモデルに入力し、テキストに変換します。CPUでも高速に動作するよう、モデルは8ビット整数に量子化して使用しています。

4.  **キーワード検出 (KWS)**: 最後に、文字起こしされたテキストを`pykakasi`というライブラリで「ひらがな」に変換します。そして、あらかじめ`config/keywords.txt`に登録しておいたNGワードの「ひらがな」と部分一致するかを照合します。漢字の「無能」とひらがなの「むのう」のように、表記ゆれがあっても検知できるのが利点です。

この一連の流れを高速に処理することで、リアルタイムでの応答を実現しています。

---

### 5. 技術解説2: LLMがログを取得・要約する仕組み (スライド5: LLM連携フロー)

続いて、LLMがどのようにして会話ログを要約するのかを説明します。

1.  **ログの監視**: `llm_worker.py`というスクリプトが、リアルタイム監視プロセスによって書き出されるログファイル (`logs/YYYY-MM-DD.txt`) を常に監視しています。

2.  **NGワード行をトリガーに起動**: ファイルに `[NG: ...] ` という文字列が追記されると、それをトリガーとして処理を開始します。

3.  **会話の切り出し（Windowing）**: `src/llm/windowing.py`が、トリガーとなったNGワードの行を基準に、その前後の会話を自動で切り出します。具体的には、「NGワードの時点から最低12秒前まで遡り、そこから合計30秒間」といったルールで、文脈を理解するのに十分な長さの会話を抽出します。

4.  **LLMへの要約依頼**: 切り出した会話のテキストを、`src/llm/client_gemini.py`を通じてGemini APIに送信します。このとき、「あなたはこの会話を要約するAIです」といった指示（プロンプト）と共に、出力してほしいJSONの形式（スキーマ）も指定します。

5.  **構造化データの保存**: Geminiから返ってきた要約、深刻度評価、推奨アクションなどが含まれるJSONデータを、`logs/summaries/`以下にJSONL形式で追記保存します。1つのインシデントが1行のJSONに対応するため、後からデータを処理しやすくなっています。

この仕組みにより、人間が介在することなく、インシデントの報告と分析が自動的に行われます。

---

### 6. デモンストレーション (スライド6: Web UI画面)

（ここで実際にWeb UIを見せる）

こちらがWebアプリケーションの画面です。
左側には日付ごとのログへのリンクがあります。
「要約」画面では、LLMが生成したインシデントのカードが並びます。カードには、検知されたNGワード、深刻度、簡単な要約、そして推奨される対応が表示されます。
「生ログ」画面では、実際の会話のやり取りを時系列で確認できます。新しい発言があると自動で更新されます。

---

### 7. まとめと今後の展望 (スライド7: まとめ)

本研究では、リアルタイム音声認識とLLMを組み合わせることで、対人業務における従業員の負担を軽減し、事後対応を効率化するシステム「CaShield」を開発しました。

今後の展望としては、
*   より高精度な音声認識モデルの導入
*   感情分析など、声のトーンを考慮した検知方法の追加
*   蓄積されたデータを用いた、ハラスメント傾向の分析機能
などを考えています。

ご清聴ありがとうございました。何かご質問はありますでしょうか。

---

## 想定Q&A

**Q1. なぜ音声認識に `faster-whisper` を選んだのですか？**

**A1.** はい、理由は2つあります。1つ目は**性能と速度のバランス**です。`faster-whisper`はOpenAIのWhisperモデルをCTranslate2というフレームワークで再実装したもので、オリジナルのWhisperよりも大幅に高速かつ省メモリで動作します。特に、`int8`量子化モデルを使えば、Raspberry Piのような非力なCPUでもリアルタイムに近い処理が可能です。2つ目は**導入の容易さ**です。Pythonライブラリとして簡単にインストールでき、特別な環境構築なしに高い認識精度を得られるため、本プロジェクトのプロトタイピングに適していると判断しました。

**Q2. LLMに会話を要約させる際のプロンプトは、どのような工夫をしていますか？**

**A2.** プロンプトでは、単に「要約して」とお願いするだけでなく、いくつかの工夫を加えています。
1.  **役割（ロール）の設定**: `あなたはカスタマーハラスメント対策の監視AIです` と最初に役割を与えることで、LLMの応答のトーンや視点を固定しています。
2.  **構造化出力の強制**: `response_schema`という機能を使って、出力してほしいJSONの形式（`ng_word`, `summary`, `severity`など）を厳密に定義しています。これにより、後続のプログラムで処理しやすい、安定した形式の出力を得られます。
3.  **事実ベースの指示**: `誇張せず、事実ベースで` といった指示を加え、LLMが創作や過度な解釈をせず、あくまでログに基づいた客観的な要約を生成するように促しています。

**Q3. リアルタイム処理とのことですが、遅延（レイテンシ）はどのくらいですか？**

**A3.** システム全体の遅延は、複数の要因によって変動します。
*   **音声入力〜VAD**: `sounddevice`と`webrtc-vad`による処理は非常に高速で、数十ミリ秒単位です。
*   **文字起こし (ASR)**: ここが最も時間がかかる部分で、話された音声の長さに依存します。数秒の発話であれば、Raspberry Pi 4のCPUでも2〜3秒程度でテキストに変換できます。
*   **LLM要約**: これは非同期のバックグラウンド処理なので、リアルタイム性には影響しません。NGワードを検知してから要約が生成されるまでには、通信時間を含めて5〜10秒程度かかります。
利用者が「検知された」と感じるまでの時間（発話してから警告音が鳴るまで）は、**おおよそ3〜5秒**程度となります。

**Q4. NGワードのリストはどのように管理していますか？表記ゆれ（例：「死ね」と「しね」）には対応できますか？**

**A4.** NGワードは `config/keywords.txt` というプレーンテキストファイルで管理しており、1行に1つの単語を記述するだけで簡単に追加・編集できます。
表記ゆれへの対応ですが、システム内部で**文字起こしされたテキストと、キーワードリストの両方を「ひらがな」に変換してから比較**しています。これにより、例えばキーワードに「馬鹿」と登録しておけば、音声認識結果が「バカ」や「ばか」であっても、すべて検知することが可能です。この処理には`pykakasi`というライブラリを利用しています。

**Q5. このシステムを実際に導入する場合、プライバシーへの配慮はどのように考えていますか？**

**A5.** 非常に重要なご指摘です。プライバシー保護は、この種のシステムにおける最重要課題の一つです。本システムでは、以下の点を考慮しています。
1.  **データ保存場所**: 全ての音声データとログは、クラウドではなく、店舗に設置したRaspberry Piのローカルストレージに保存されます。これにより、第三者へのデータ漏洩リスクを最小限に抑えます。
2.  **ログの目的外利用の制限**: ログはあくまでハラスメント発生時の証拠保全と事後対応の効率化を目的とするものであり、従業員の監視や評価に用いるべきではありません。導入する際には、明確な運用ルールを定め、全従業員に周知することが不可欠です。
3.  **告知の徹底**: 店舗の入り口やレジ前など、録音対象となるエリアのお客様に対して、「防犯と応対品質向上のため、会話を記録させていただく場合があります」といったステッカーや掲示で明確に告知することが重要です。

将来的には、音声データをテキスト化後に即時破棄するオプションや、個人名を自動でマスキングする機能なども検討すべきだと考えています。

**Q6. PyAudioなど他の音声ライブラリもある中で、sounddeviceとwebrtc-vadを特に選んだ理由は何ですか？**

**A6.** 良いご質問ありがとうございます。その2つを選んだのは、**リアルタイム性と効率性**を最大限に高めるためです。
*   **sounddevice**: PyAudioも優れたライブラリですが、`sounddevice`は特に低遅延（ローレイテンシ）な音声ストリーミングに強みがあります。本システムでは`RawInputStream`という機能を使って、OSのバッファを介さず、非常に小さな単位（20-30ミリ秒）で直接マイクから音声データを取得しています。これにより、発話からシステムが反応するまでの時間を極限まで短縮し、リアルタイム性を高めています。
*   **webrtc-vad**: こちらはGoogleが開発した、軽量かつ高性能な音声区間検出（Voice Activity Detection）ライブラリです。人の声がする区間だけを正確に切り出し、無音の部分を処理から除外する役割を担います。これにより、後段の音声認識エンジン（faster-whisper）に無駄なデータを送らずに済み、計算リソースを節約できるだけでなく、認識精度そのものの向上にも繋がります。
この2つを組み合わせることで、「**素早く音声を受け取り、話している部分だけを効率的に処理する**」という、本システムの根幹をなすパイプラインを実現しています。